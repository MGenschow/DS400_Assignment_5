---
title: "Assignment 5"
author: "Malte Genschow"
date: "24 12 2021"
output: html_document
---
#### Code of Conduct
You will only successfully complete this assignment if you adhere to the following rules and guidelines. You
are allowed to work in groups, however,

* the work submitted must be yours.
* the code must be your own.
* you must make clear whom you worked with.

**By submitting this assignment I agree to the above mentioned terms.**

### General Setup:

##### Clear Workspace
```{r, }
rm(list = ls())
```

### 1. GitHub
The code for this assignment can be found in [this repository](https://github.com/MGenschow/DS400_Assignment_5).

### 2. Getting to know the API

* The rate limits for the [Ticketmaster Discovery API](https://developer.ticketmaster.com/products-and-docs/apis/getting-started/) is 5000 API calls per day and rate limitation of 5 requests per second. 

* An API key has been obtained from the [API Explorer](https://developer.ticketmaster.com/api-explorer/v2/) and saved in a separate document. This document was added to the.gitignore file such that the secret key is not shared on Github. This key is sourced in the following code:

```{r}
source('ticketmaster_secret_key.R')
```


### 3. Basic Interacting with the API
##### Load the required packages
```{r, results = 'hide', warning=FALSE, message=FALSE}
library(httr)
library(jsonlite)
library(dplyr)
library(ggplot2)
```

##### Create Endpoint URL 
```{r}
country_code = 'DE'
url = paste('https://app.ticketmaster.com/discovery/v2/venues?apikey=', key, '&locale=*&countryCode=' , country_code, sep='')
url
```

##### Get content from API and inspect
```{r}
response = GET(url)
content = content(response)
```
The resulting list of three contains all the information for 20 venues obtained from the API. This list of lists contains the information about the venue in the first entry _embedded_ -> _venues_. In each of the 20 entries, one can find the name, city, postalCode, address, url, longitude, latitude and many more. The second entry ( _links_ ) contains links to the current page, the next page and the last page for all the venues on the api. The last entry ( _page_) contains information about the number of entries at the current page, the number of entries in general (on all pages) and the total number of pages.

In the following only the relevant information per venue is extracted from the nested list:
```{r}
json_content = content(response, as='text')
venue_data = data.frame(fromJSON(json_content)[[1]][[1]])
venue_data = as.data.frame(venue_data[,c('name', 'city', 'postalCode', 'address', 'url', 'location')])
venue_data = do.call(data.frame, venue_data)
venue_data=venue_data %>% rename(
  city = name.1,
  address = line1, 
  longitude = location.longitude,
  latitude = location.latitude
)
glimpse(venue_data)
```


### Advanced Interaction with the API
```{r}
content[[3]]
```

Looking at the content of the above API call, it becomes clear that the results are only the first 20 results of 12568 venues in total, i.e. the first page of 629 pages (when leaving the size at the default, which is 20). The API documentation however also states that a query parameter _size_ can be chosen, such that more than 20 entries per API call can be retrieved. Furthermore, the page can be stated explicitly in the construction of the API endpoint URl such that one can simply loop over the different pages to retrieve all the data.


1. In order to construct the loop, one has to find the number of pages for a given page size. This will be the running index in the loop that will be constructed later.
```{r}
country_code = 'DE'
size = 500
url = paste('https://app.ticketmaster.com/discovery/v2/venues?apikey=', key, '&locale=*&size=',size,'&countryCode=' , country_code, sep='')
response = GET(url)
content = content(response)
total_pages = content[[3]][[3]]
total_pages
```

2. Now the loop can be constructed. Data cleaning will happen inside the loop, such that in each iteration, 500 new entries will be brought into tidy format and appended to the final dataframe.
```{r}
all_data = data.frame()
for (page in 0:(total_pages-1)){
  url = paste('https://app.ticketmaster.com/discovery/v2/venues?apikey=', key, '&locale=*&size=',size,'&page=',page,'&countryCode=' , country_code, sep='')
  response = GET(url)
  #parse to json
  content = content(response)
  json_content = content(response, as='text')
  # parse to r-dataframe 
  venue_data = data.frame(fromJSON(json_content)[[1]][[1]])
  # subset to columns needed
  venue_data = as.data.frame(venue_data[,c('name', 'city', 'postalCode', 'address', 'url', 'location')])
  # collapse nested structure in the location (lat and lon)
  venue_data = do.call(data.frame, venue_data)
  # rename columns
  venue_data= venue_data %>% rename(
  city = name.1,
  longitude = location.longitude,
  latitude = location.latitude
  )
  # rename address columns conditionally
  if ('line1' %in% names(venue_data))
    venue_data= venue_data %>% rename(
      address = line1)
  if ('address.line1' %in% names(venue_data))
    venue_data= venue_data %>% rename(
      address = address.line1)
  # combine all data together
  all_data = bind_rows(all_data, venue_data)
  # Add sleep in each iteration such that rate limits are not exceeded
  Sys.sleep(0.4)
}
```

Quick look at the resulting data
```{r}
all_data$address.line2 = NULL
all_data[,c('postalCode', 'longitude', 'latitude')] = sapply(all_data[,c('postalCode', 'longitude', 'latitude')], as.numeric)
glimpse(all_data)
```
### Visualizing the extracted data
```{r}
# Exclude obvious faulty coordinates
is.na(all_data['longitude']) <-  all_data['longitude'] < 5.866944 | all_data['longitude'] >15.043611
is.na(all_data['latitude']) <-  all_data['latitude'] < 47.271679 | all_data['latitude'] >55.0846
all_data$latitude = ifelse(is.na(all_data$longitude),NA,all_data$latitude)
all_data$longitude = ifelse(is.na(all_data$latitude),NA,all_data$longitude)
```


```{r}
# plot the data on a map
ggplot() +
  geom_polygon(
  aes(x = long, y = lat, group = group), data = map_data("world", region = "Germany"),
    fill = "grey90",color = "black") +
  theme_void() + coord_quickmap() +
  labs(title = "Event locations across Germany", caption = "Source: ticketmaster.com") +
  theme(title = element_text(size=8, face='bold'),
  plot.caption = element_text(face = "italic"))+
  geom_point(aes(x=longitude, y=latitude), data=all_data, size=0.4, color='red', na.rm=T)
```

### Repeat the task for Spain
```{r}
country_code = 'ES'
size = 500
url = paste('https://app.ticketmaster.com/discovery/v2/venues?apikey=', key, '&locale=*&size=',size,'&countryCode=' , country_code, sep='')
response = GET(url)
content = content(response)
total_pages = content[[3]][[3]]
total_pages

all_data = data.frame()
for (page in 0:(total_pages-1)){
  url = paste('https://app.ticketmaster.com/discovery/v2/venues?apikey=', key, '&locale=*&size=',size,'&page=',page,'&countryCode=' , country_code, sep='')
  response = GET(url)
  #parse to json
  content = content(response)
  json_content = content(response, as='text')
  # parse to r-dataframe 
  venue_data = data.frame(fromJSON(json_content)[[1]][[1]])
  # subset to columns needed
  venue_data = as.data.frame(venue_data[,c('name', 'city', 'postalCode', 'address', 'url', 'location')])
  # collapse nested structure in the location (lat and lon)
  venue_data = do.call(data.frame, venue_data)
  # rename columns
  venue_data= venue_data %>% rename(
  city = name.1,
  longitude = location.longitude,
  latitude = location.latitude
  )
  # rename address columns conditionally
  if ('line1' %in% names(venue_data))
    venue_data= venue_data %>% rename(
      address = line1)
  if ('address.line1' %in% names(venue_data))
    venue_data= venue_data %>% rename(
      address = address.line1)
  # combine all data together
  all_data = bind_rows(all_data, venue_data)
  # Add sleep in each iteration such that rate limits are not exceeded
  Sys.sleep(0.4)
}
```

Quick look at the resulting data
```{r}
all_data$address.line2 = NULL
all_data[,c('postalCode', 'longitude', 'latitude')] = sapply(all_data[,c('postalCode', 'longitude', 'latitude')], as.numeric)
glimpse(all_data)
```

### Visualizing the extracted data
The extremum coordinates used below can be found [here](https://en.wikipedia.org/wiki/List_of_extreme_points_of_Spain)
```{r}
# Exclude obvious faulty coordinates
is.na(all_data['longitude']) <-  all_data['longitude'] < -18.15 | all_data['longitude'] >4.316667
is.na(all_data['latitude']) <-  all_data['latitude'] < 27.633333 | all_data['latitude'] >43.783333
all_data$latitude = ifelse(is.na(all_data$longitude),NA,all_data$latitude)
all_data$longitude = ifelse(is.na(all_data$latitude),NA,all_data$longitude)
```


```{r}
# plot the data on a map
ggplot() +
  geom_polygon(
  aes(x = long, y = lat, group = group), data = map_data("world", region = "Spain"),
    fill = "grey90",color = "black") +
  theme_void() + coord_quickmap() +
  labs(title = "Event locations across Spain", caption = "Source: ticketmaster.com") +
  theme(title = element_text(size=8, face='bold'),
  plot.caption = element_text(face = "italic")) +
  geom_point(aes(x=longitude, y=latitude), data=all_data, size=0.4, color='red', na.rm=T)
```











